{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c230d91a",
   "metadata": {},
   "source": [
    "## Now that dependencies are installed, ready to move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17affc47",
   "metadata": {},
   "source": [
    "## Log into huggingface\n",
    "Run the cell below and paste your token into the prompt.  You can get your token from your [huggingface account page](https://huggingface.co/settings/tokens).\n",
    "\n",
    "The token will not show on the screen, just press enter after you paste it.\n",
    "\n",
    "Then run the following cell to download the base checkpoint (may take a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo=\"panopstor/EveryDream\"\n",
    "ckpt_file=\"sd_v1-5_vae.ckpt\"\n",
    "\n",
    "print(f\"Downloading {ckpt_file} from {repo}\")\n",
    "downloaded_model_path = hf_hub_download(repo, ckpt_file, cache_dir=\"/workspace/hfcache\")\n",
    "!cp -L {downloaded_model_path} /workspace/EveryDream2trainer\n",
    "\n",
    "ckpt_name = os.path.splitext(ckpt_file)[0]\n",
    "\n",
    "if not os.path.exists(f\"ckpt_cache/{ckpt_name}\"):\n",
    "    !python utils/convert_original_stable_diffusion_to_diffusers.py --scheduler_type ddim \\\n",
    "    --original_config_file v1-inference.yaml \\\n",
    "    --image_size 512 \\\n",
    "    --checkpoint_path \"{ckpt_file}\" \\\n",
    "    --prediction_type epsilon \\\n",
    "    --upcast_attn False \\\n",
    "    --dump_path \"ckpt_cache/{ckpt_name}\"\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e7cb7-4250-4f52-842f-89d2152c29c1",
   "metadata": {},
   "source": [
    "# Start Training\n",
    "Naming your project will help you track what the heck you're doing when you're floating in checkpoint files later.\n",
    "\n",
    "You may wish to consider adding \"sd1\" or \"sd2v\" or similar to remember what the base was, as you'll also have to tell your inference app what you were using, as its difficult for programs to know what inference YAML to use automatically. For instance, Automatic1111 webui requires you to copy the v2 inference YAML and rename it to match your checkpoint name so it knows how to load the file, tough it assumes SD 1.x compatible. Something to keep in mind if you start training on SD2.1.\n",
    "\n",
    "Next cell runs training. This will take a while depending on your number of images, repeats, and max_epochs.\n",
    "\n",
    "You can watch for test images in the logs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8a052-bc26-4339-81e0-3b4f78c1d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"finetune_v1\"\n",
    "\n",
    "!python train.py --project_name \"{project_name}_384_35e6\" \\\n",
    "--resume_ckpt \"{ckpt_name}\" \\\n",
    "--data_root \"input\" \\\n",
    "--resolution 384 \\\n",
    "--batch_size 5 \\\n",
    "--max_epochs 100 \\\n",
    "--save_every_n_epochs 9999 \\\n",
    "--lr 3.5e-6 \\\n",
    "--lr_scheduler cosine \\\n",
    "--sample_steps 100 \\\n",
    "--ed1_mode \\\n",
    "--useadam8bit \\\n",
    "--save_full_precision \\\n",
    "--shuffle_tags \\\n",
    "--notebook\n",
    "\n",
    "!python train.py --project_name \"{project_name}_512_25e6\" \\\n",
    "--resume_ckpt \"findlast\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 50 \\\n",
    "--ckpt_every_n_minutes 60 \\\n",
    "--resolution 512 \\\n",
    "--batch_size 4 \\\n",
    "--gradient_checkpointing \\\n",
    "--lr 2.5e-6 \\\n",
    "--lr_scheduler cosine \\\n",
    "--sample_steps 200 \\\n",
    "--ed1_mode \\\n",
    "--useadam8bit \\\n",
    "--save_full_precision \\\n",
    "--shuffle_tags \\\n",
    "--notebook\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d6247-1d51-4f70-ab09-f29ddff9516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --project_name \"{project_name}_512_25e6\" \\\n",
    "--resume_ckpt \"findlast\" \\\n",
    "--data_root \"input\" \\\n",
    "--save_every_n_epochs  \\\n",
    "--resolution 512 \\\n",
    "--batch_size 4 \\\n",
    "--gradient_checkpointing \\\n",
    "--max_epochs 10 \\\n",
    "--lr 2.5e-6 \\\n",
    "--lr_scheduler cosine \\\n",
    "--sample_steps 200 \\\n",
    "--ed1_mode \\\n",
    "--useadam8bit \\\n",
    "--save_full_precision \\\n",
    "--shuffle_tags \\\n",
    "--notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d37a1",
   "metadata": {},
   "source": [
    "# HuggingFace upload\n",
    "Use the cell below to upload your checkpoint to your personal HuggingFace account if you want instead of manually downloading. You should already be authorized to Huggingface by token if you used the download/token cells above.\n",
    "\n",
    "Make sure to fill in the three fields at the top. This will only upload one file at a time, so you will need to run the cell below for each file you want to upload.\n",
    "\n",
    "* You can get your account name from your [HuggingFace account page](https://huggingface.co/settings/account). Look for your \"username\" field and paste it below.\n",
    "\n",
    "* You only need to setup a repository one time.  You can create it here: [Create New HF Model](https://huggingface.co/new)  Make sure you write down the repo name you make for future use.  You can reuse it later.\n",
    "\n",
    "* You need to type the name of the ckpts one at a time in the cell below, find them in the left file drawer of your Runpod/Vast/Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb962e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list ckpts in root that are ready for download\n",
    "import glob\n",
    "\n",
    "for f in glob.glob(\"*.ckpt\"):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7237ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli lfs-enable-largefiles\n",
    "# fill in these three fields:\n",
    "hfusername = \"MyHfUser\"\n",
    "reponame = \"MyRepo\"\n",
    "ckpt_name = \"finetune_v1-ep10-gs02500.ckpt\"\n",
    "\n",
    "\n",
    "target_name = ckpt_name.replace('-','').replace('=','')\n",
    "import os\n",
    "os.rename(ckpt_name,target_name)\n",
    "repo_id=f\"{hfusername}/{reponame}\"\n",
    "print(f\"uploading to HF: {repo_id}/{ckpt_name}\")\n",
    "print(\"this make take a while...\")\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "response = api.upload_file(\n",
    "    path_or_fileobj=target_name,\n",
    "    path_in_repo=target_name,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=None,\n",
    "    create_pr=1,\n",
    ")\n",
    "print(response)\n",
    "print(finish_msg)\n",
    "print(\"go to your repo and accept the PR this created to see your file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b54d4-6f6f-491d-9c1f-9b5566718d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
